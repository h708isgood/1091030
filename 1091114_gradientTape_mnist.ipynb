{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1091114_gradientTape_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIfMsDUxMvH5GasnM8m6rT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h708isgood/1091030/blob/1091031/1091114_gradientTape_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAfx03Z__I4d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SviTsvn__Gtx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
        "y_train = y_train.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "x_train=x_train.reshape(-1,784)\n",
        "x_test=x_test.reshape(-1,784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3m0wGqCF8tg"
      },
      "source": [
        "batch_size = 64 #60000/64=#937.5 ===938\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "train_dataset2 = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "test_dataset2 = test_dataset.shuffle(buffer_size=1024).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maturs8bGe8v"
      },
      "source": [
        "# x_batch_train.shape (64,28,28) 每次64筆\n",
        "for step, (x_batch_train, y_batch_train) in enumerate(train_dataset2):\n",
        "    print(x_batch_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2YIOEthLq__"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkxH3xDpEyPu"
      },
      "source": [
        "model = tf.keras.models.Sequential()                 #← 建立序列模型物件\n",
        "model.add(layers.Dense(512, activation='relu', input_dim= 784)) #← 加入第一層\n",
        "model.add(layers.Dense(10, activation='softmax')) \n",
        "\n",
        "adam=tf.keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,\n",
        "         epsilon=1e-07,decay=0, amsgrad=False)             \n",
        "model.compile(optimizer=adam,             #← 指定優化器\n",
        "      loss=loss_fn,  #'sparse_categorical_crossentropy', #← 指定損失函數\n",
        "      metrics=['acc'])\n",
        "model2 = tf.keras.models.Sequential()                 #← 建立序列模型物件\n",
        "model2.add(layers.Dense(512, activation='relu', input_dim= 784)) #← 加入第一層\n",
        "model2.add(layers.Dense(10, activation='softmax')) \n",
        "\n",
        "adam=tf.keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999,\n",
        "         epsilon=1e-07,decay=0, amsgrad=False)             \n",
        "model2.compile(optimizer=adam,             #← 指定優化器\n",
        "      loss=loss_fn,  #'sparse_categorical_crossentropy', #← 指定損失函數\n",
        "      metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyaz5s7WTXbA"
      },
      "source": [
        "model2.fit(x_train, y_train, epochs=10, batch_size=64)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RH6u7EEUO1w",
        "outputId": "a58b6988-6c4d-4295-dbca-57681b6a4d99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model2.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0785 - acc: 0.9794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07851317524909973, 0.9793999791145325]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY1p4UTaGo6Q",
        "outputId": "c7f1d217-0b46-4869-bf27-5e12b8a2ec72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def step_():\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset2):\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(x_batch_train)\n",
        "      loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "    #if step % 200 == 0:\n",
        "    #    print('Training loss (for one batch) at step %s: %s'\n",
        "    #         % (step, float(loss_value)))\n",
        "    #    print('Seen so far: %s samples' % ((step + 1) * 64))\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    adam.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    #train_acc_metric(y_batch_train, logits)\n",
        "    train_acc_metric.update_state(y_batch_train, logits)\n",
        "  \n",
        "for epoch in range(10):\n",
        "    step_()\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "    train_acc_metric.reset_states()\n",
        "    print(model.evaluate(x_test, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training acc over epoch: 0.9484500288963318\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1037 - acc: 0.9670\n",
            "[0.10366448014974594, 0.9670000076293945]\n",
            "Training acc over epoch: 0.9795166850090027\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0796 - acc: 0.9760\n",
            "[0.07958237826824188, 0.9760000109672546]\n",
            "Training acc over epoch: 0.9874833226203918\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0784 - acc: 0.9751\n",
            "[0.07844088226556778, 0.9750999808311462]\n",
            "Training acc over epoch: 0.9911666512489319\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0663 - acc: 0.9807\n",
            "[0.0662819966673851, 0.9807000160217285]\n",
            "Training acc over epoch: 0.9943666458129883\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0597 - acc: 0.9830\n",
            "[0.059709496796131134, 0.9829999804496765]\n",
            "Training acc over epoch: 0.9964166879653931\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0664 - acc: 0.9812\n",
            "[0.06644991785287857, 0.9811999797821045]\n",
            "Training acc over epoch: 0.9973166584968567\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0702 - acc: 0.9808\n",
            "[0.07016681879758835, 0.9807999730110168]\n",
            "Training acc over epoch: 0.9973000288009644\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0675 - acc: 0.9812\n",
            "[0.06754332035779953, 0.9811999797821045]\n",
            "Training acc over epoch: 0.9979000091552734\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0727 - acc: 0.9807\n",
            "[0.07271303981542587, 0.9807000160217285]\n",
            "Training acc over epoch: 0.9987000226974487\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0755 - acc: 0.9802\n",
            "[0.07551714777946472, 0.9801999926567078]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8964yjEPSNQ",
        "outputId": "e00d2a2b-16bd-4ca6-fcbb-ac2f86baea80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cc=0\n",
        "for x_batch_val, y_batch_val in test_dataset :\n",
        "  cc +=1\n",
        "\n",
        "sum=0\n",
        "for x_batch_val, y_batch_val in test_dataset2 :\n",
        "    val_logits = model(x_batch_val)\n",
        "    val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    \n",
        "    #sum +=val_acc\n",
        "    #print('Validation acc: %s' % (float(val_acc),))\n",
        "#print(sum/cc)\n",
        "val_acc = val_acc_metric.result()\n",
        "val_acc_metric.reset_states()\n",
        "print(val_acc)\n",
        "model.evaluate(x_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.9802, shape=(), dtype=float32)\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0755 - acc: 0.9802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07551714777946472, 0.9801999926567078]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVWWT9WdFSoC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}